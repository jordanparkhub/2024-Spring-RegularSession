{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNfkrXYFaPqpewrYqYyXcvz"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Tokenizing\n","\n","이름 :\n","\n","기수 :\n","\n","작성자 : 10 기 신재우"],"metadata":{"id":"EsiMEkOD-Be_"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ycGe9djWG8dr","executionInfo":{"status":"ok","timestamp":1708676535980,"user_tz":-540,"elapsed":22635,"user":{"displayName":"Jae Woo Shin","userId":"00227897202452450211"}},"outputId":"8d59fa8f-1f54-4ed3-a52a-1e4ee37f180f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["# colab 환경에서 학습을 진행하실 분들은 구글드라이브를 연동해주세요\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","source":["### 시작 전 실행할 것들"],"metadata":{"id":"G65-th2N-Axi"}},{"cell_type":"code","source":["%pip install konlpy"],"metadata":{"id":"QvlKchYJDQDP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from konlpy.tag import Kkma\n","from konlpy.utils import pprint\n","import pandas as pd\n","import numpy as np\n","import tqdm\n","import spacy\n","import torchtext\n","import torch\n","from torch.utils.data import DataLoader\n","import torch.nn.functional as F\n","import torch.nn as nn\n","import random\n","import os"],"metadata":{"id":"SNFDr3NwDRDQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 경로 설정"],"metadata":{"id":"K92VL0XGDTVS"}},{"cell_type":"code","source":["path_to_folder = \"????\""],"metadata":{"id":"kWuka-VrDVK5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["path_to_dataset = os.path.join(path_to_folder, \"Dataset/1_구어체(1).xlsx\")"],"metadata":{"id":"su1E9a3VDb6I"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset = pd.read_excel(path_to_dataset)\n","dataset = dataset[['원문', '번역문']]"],"metadata":{"id":"-JF6jXBSDR2q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_dataset = dataset.iloc[:30000, :]\n","val_dataset = dataset.iloc[30000:35000, :]\n","test_dataset = dataset.iloc[35000:40000, :]\n","\n","kr_tokenizer = Kkma().morphs\n","en_tokenizer = spacy.load(\"en_core_web_sm\").tokenizer"],"metadata":{"id":"KT_DgcZ8ENT1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for i in en_tokenizer(\"Hello, my name is Jae Woo Shin!\"):\n","  print(i)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xPQ90vNEHz8M","executionInfo":{"status":"ok","timestamp":1708677729012,"user_tz":-540,"elapsed":3,"user":{"displayName":"Jae Woo Shin","userId":"00227897202452450211"}},"outputId":"59746716-1615-4320-a820-9f0752963e83"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Hello\n",",\n","my\n","name\n","is\n","Jae\n","Woo\n","Shin\n","!\n"]}]},{"cell_type":"code","source":["kr_tokenizer(\"안녕하세요 저는 신재우에요.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lQG3JQjsTGAs","executionInfo":{"status":"ok","timestamp":1708680579999,"user_tz":-540,"elapsed":3,"user":{"displayName":"Jae Woo Shin","userId":"00227897202452450211"}},"outputId":"820f3286-5ad3-4192-d2f4-c5d042cf60bf"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['안녕', '하', '세요', '저', '는', '신', '재우', '에', '요', '.']"]},"metadata":{},"execution_count":39}]},{"cell_type":"code","source":["for i in kr_tokenizer(\"안녕하세요 저는 신재우에요.\"):\n","  print(i)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6Z1k6IilHipv","executionInfo":{"status":"ok","timestamp":1708677731672,"user_tz":-540,"elapsed":3,"user":{"displayName":"Jae Woo Shin","userId":"00227897202452450211"}},"outputId":"d007c435-6515-4eb7-e3b2-9b11785e34de"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["안녕\n","하\n","세요\n","저\n","는\n","신\n","재우\n","에\n","요\n",".\n"]}]},{"cell_type":"code","source":["sos_token = \"<sos>\"\n","eos_token = \"<eos>\""],"metadata":{"id":"hdXIwy5NEOSE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 문제 1 :\n","\n","아래 ___ 으로 되어 있는 코드를 채워주세요."],"metadata":{"id":"nScLemvMNtOa"}},{"cell_type":"code","source":["def tokenize(row, en_tokenizer, kr_tokenizer, max_length, sos_token, eos_token):\n","  \"\"\"\n","  여기에서의 목표는 Input 를 row 로 받아서 이것을 Tokenizer 를 통해서 벡터로 바꾸는 것이 목표입니다.\n","\n","  row : 우리들의 input, Dataframe 형태로 되어 있으며 영어 텍스트는 row[\"번역문\"], 한국어 텍스는 row[\"원문\"] 으로 되어 있습니다.\n","  en_tokenizer : 영어 텍스트를 토큰으로 바꿔주는 토크나이저, 실행 방법은 en_tokenizer(en_text)\n","  kr_tokenizer : 한국어 텍스트를 토큰으로 바꿔주는 토크나이저, 실행 방법은 kr_tokenizer(kr_text)\n","\n","  max_length : 1000 이며, 각 텍스트마다 토큰들이 이 개수를 넘기면 안되며, 이것을 넘는 토큰은 버린다\n","  sos_token : 시작 토큰, <sos>\n","  eos_token : 끝 토큰, <eos>\n","\n","  return : en_tokens, kr_tokens, 각각 앞과 끝에 sos_token 과 eos_token 이 있어야 하는 각 텍스트를 토큰 형태로 바꿔준 리스트다.\n","\n","  힌트 :\n","  - List Comprehension 을 통해서 텍스트에 있는 토큰들을 토큰화 시킨 것들을 리스트로 저장해줍니다.\n","\n","  참고 : 영어의 경우 모두 다 소문자로 바꿔주세요!\n","  \"\"\"\n","  en_text = row[\"번역문\"]\n","  kr_text = row[\"원문\"]\n","\n","  # 영어의 경우 모두 다 소문자로 바꿔주세요! 학습에 도움이 되기 위함입니다.\n","  en_tokens = ___\n","  kr_tokens = ___\n","\n","  final_en_tokens = [sos_token] + en_tokens + [eos_token]\n","  final_kr_tokens = [sos_token] + kr_tokens + [eos_token]\n","\n","  return pd.Series({\"en_tokens\": final_en_tokens, \"kr_tokens\": final_kr_tokens})"],"metadata":{"id":"R9CcgcejDkSA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from tqdm import tqdm\n","tqdm.pandas()\n","\n","max_length = 1000\n","\n","train_dataset[[\"en_tokens\", \"kr_tokens\"]] = train_dataset.progress_apply(tokenize, axis=1, args=(nlp, kkma, max_length, sos_token, eos_token))\n","val_dataset[[\"en_tokens\", \"kr_tokens\"]] = val_dataset.progress_apply(tokenize, axis=1, args=(nlp, kkma, max_length, sos_token, eos_token))\n","test_dataset[[\"en_tokens\", \"kr_tokens\"]]= test_dataset.progress_apply(tokenize, axis=1, args=(nlp, kkma, max_length, sos_token, eos_token))"],"metadata":{"id":"4e2C2JbqEZew"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 위의 코드가 진행이 굉장히 오래되며, 이것을 오래 기다리기에는 힘들기에 저장을 시켜줘서 다음 파일인 Seq2Seq.ipynb 에서 진행하겠습니다"],"metadata":{"id":"HZll_6hGN2tm"}},{"cell_type":"code","source":["path_train = os.path.join(path_to_folder, \"Dataset/train_dataset_Tokenized.csv\")\n","path_val = os.path.join(path_to_folder, \"Dataset/val_dataset_Tokenized.csv\")\n","path_test = os.path.join(path_to_folder, \"Dataset/test_dataset_Tokenized.csv\")"],"metadata":{"id":"BroVtAGOUeHk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_dataset.to_csv(path_train)\n","val_dataset.to_csv(path_val)\n","test_dataset.to_csv(path_test)"],"metadata":{"id":"-Kawz4vbEuDb"},"execution_count":null,"outputs":[]}]}