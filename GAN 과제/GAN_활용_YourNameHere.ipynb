{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# **GAN 활용 코드 과제**\n","---\n","- 출제자 : 장현빈\n","- 기수/이름 :"],"metadata":{"id":"Cy9XgtbjUAvw"}},{"cell_type":"markdown","source":["pre-trained 된 모델과 CLIP 모델을 이용하여 StyleGAN을 실행시켜보고,\\\n","image to image translation이 이루어지는 과정을 확인해봅시다!"],"metadata":{"id":"3YhjeMRRXxCx"}},{"cell_type":"markdown","source":["##**StyleGAN image to image translation 실습**"],"metadata":{"id":"2nQyGZonUXhm"}},{"cell_type":"markdown","source":["###**라이브러리 임포트**"],"metadata":{"id":"vV4y-6XuUu3K"}},{"cell_type":"code","source":["!git clone https://github.com/NVlabs/stylegan3 -q\n","!git clone https://github.com/openai/CLIP -q\n","!pip install -e ./CLIP -q\n","!pip install einops ninja -q"],"metadata":{"id":"DOkwpbbzVC0E"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import sys\n","sys.path.append('./CLIP')\n","sys.path.append('./stylegan3')\n","\n","import io\n","import os, time\n","import pickle\n","import shutil\n","import numpy as np\n","from PIL import Image\n","import torch\n","import torch.nn.functional as F\n","import requests\n","import torchvision.transforms as transforms\n","import torchvision.transforms.functional as TF\n","import clip\n","from tqdm.notebook import tqdm\n","from torchvision.transforms import Compose, Resize, ToTensor, Normalize\n","from IPython.display import display\n","from einops import rearrange\n","from google.colab import files"],"metadata":{"id":"tf_ePG9LUHof"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["device = torch.device('cuda')\n","print('Using device:', device, file=sys.stderr)"],"metadata":{"id":"k-EAL3_TVPHe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###**CLIP 모델로 image embedding 함수 정의**"],"metadata":{"id":"39YqzRnJVnEy"}},{"cell_type":"code","source":["def fetch(url_or_path):\n","    if str(url_or_path).startswith('http://') or str(url_or_path).startswith('https://'):\n","        r = requests.get(url_or_path)\n","        r.raise_for_status()\n","        fd = io.BytesIO()\n","        fd.write(r.content)\n","        fd.seek(0)\n","        return fd\n","    return open(url_or_path, 'rb')\n","\n","def fetch_model(url_or_path):\n","    basename = os.path.basename(url_or_path)\n","    if os.path.exists(basename):\n","        return basename\n","    else:\n","        !wget -c '{url_or_path}'\n","        return basename\n","\n","def norm1(prompt):\n","    \"Normalize to the unit sphere.\"\n","    return prompt / prompt.square().sum(dim=-1,keepdim=True).sqrt()\n","\n","def spherical_dist_loss(x, y):\n","    x = F.normalize(x, dim=-1)\n","    y = F.normalize(y, dim=-1)\n","    return (x - y).norm(dim=-1).div(2).arcsin().pow(2).mul(2)\n","\n","class MakeCutouts(torch.nn.Module):\n","    def __init__(self, cut_size, cutn, cut_pow=1.):\n","        super().__init__()\n","        self.cut_size = cut_size\n","        self.cutn = cutn\n","        self.cut_pow = cut_pow\n","\n","    def forward(self, input):\n","        sideY, sideX = input.shape[2:4]\n","        max_size = min(sideX, sideY)\n","        min_size = min(sideX, sideY, self.cut_size)\n","        cutouts = []\n","        for _ in range(self.cutn):\n","            size = int(torch.rand([])**self.cut_pow * (max_size - min_size) + min_size)\n","            offsetx = torch.randint(0, sideX - size + 1, ())\n","            offsety = torch.randint(0, sideY - size + 1, ())\n","            cutout = input[:, :, offsety:offsety + size, offsetx:offsetx + size]\n","            cutouts.append(F.adaptive_avg_pool2d(cutout, self.cut_size))\n","        return torch.cat(cutouts)\n","\n","make_cutouts = MakeCutouts(224, 32, 0.5)\n","\n","def embed_image(image):\n","  n = image.shape[0]\n","  cutouts = make_cutouts(image)\n","  embeds = clip_model.embed_cutout(cutouts)\n","  embeds = rearrange(embeds, '(cc n) c -> cc n c', n=n)\n","  return embeds\n","\n","def embed_url(url):\n","  image = Image.open(fetch(url)).convert('RGB')\n","  return embed_image(TF.to_tensor(image).to(device).unsqueeze(0)).mean(0).squeeze(0)\n","\n","class CLIP(object):\n","  def __init__(self):\n","    clip_model = \"ViT-B/32\"\n","    self.model, _ = clip.load(clip_model)\n","    self.model = self.model.requires_grad_(False)\n","    self.normalize = transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073],\n","                                          std=[0.26862954, 0.26130258, 0.27577711])\n","\n","  @torch.no_grad()\n","  def embed_text(self, prompt):\n","      \"Normalized clip text embedding.\"\n","      return norm1(self.model.encode_text(clip.tokenize(prompt).to(device)).float())\n","\n","  def embed_cutout(self, image):\n","      \"Normalized clip image embedding.\"\n","      return norm1(self.model.encode_image(self.normalize(image)))\n","\n","clip_model = CLIP()"],"metadata":{"id":"v9R5Kj7XVscw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###**사용할 pre-trained styleGAN 모델 선택하기**"],"metadata":{"id":"cKpOoli-Vw9c"}},{"cell_type":"markdown","source":["**아래의 pre-trained model 중 하나를 선택하여, *'model_name'*에 입력해주세요!**\n","\n","1) ffhq: 얼굴 사진으로 pre-trained\n","- \"stylegan3-r-ffhq-1024x1024.pkl\"\n","- \"stylegan3-t-ffhq-1024x1024.pkl\"\n","\n","2) metfaces: 명화 pre-trained\n","- \"stylegan3-r-metfaces-1024x1024.pkl\"\n","- \"stylegan3-t-metfaces-1024x1024.pkl\"\n","\n","3) afhqv2: 동물 pre-trained\n","\n","- \"stylegan3-r-afhqv2-512x512.pkl\"\n","- \"stylegan3-t-afhqv2-512x512.pkl\"\n"],"metadata":{"id":"zv-sCNMGWfH-"}},{"cell_type":"code","source":["base_url = \"https://api.ngc.nvidia.com/v2/models/nvidia/research/stylegan3/versions/1/files/\"\n","\n","model_name = \"stylegan3-r-ffhq-1024x1024.pkl\"\n","network_url = base_url + model_name\n","\n","with open(fetch_model(network_url), 'rb') as fp:\n","  G = pickle.load(fp)['G_ema'].to(device)\n","\n","clip_model = CLIP()"],"metadata":{"id":"IPBSaI_ZV5SG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###**파라미터 설정**"],"metadata":{"id":"0FP0LxVnY1s6"}},{"cell_type":"markdown","source":["*text = '???'* 변수에 단어, 구 등을 작성합니다.\\\n","ex. 유명 인물 이름, 캐릭터 이름, 동물 이름 등\n","- pre-trained 모델로 선택한 종류를 기반으로 작성해야 더 좋은 이미지가 생성됩니다!\n","- 사용할 모델을 변경하려는 경우 위의 모델 선택 코드를 다시 수행해주셔야 합니다!\n","\n","\n","*steps*: epoch 수 지정\\\n","*show_every_n_steps*: epoch 몇 번 당 한 번 씩 생성 image를 출력할지 지정"],"metadata":{"id":"VMDjAMWBYIAe"}},{"cell_type":"code","source":["text = 'Barack Obama' # 이걸 바꿔주면 됩니다!\n","target = clip_model.embed_text(text)\n","\n","steps = 100\n","\n","target_images = \"em.jpg\"\n","\n","seed =  -1  #-1 for completly random\n","if seed == -1:\n","    seed = np.random.randint(0,2**32 - 1)\n","\n","show_every_n_steps = 20\n","\n","zs = torch.randn([10000, G.mapping.z_dim], device=device)\n","w_stds = G.mapping(zs, None).std(0)\n","\n","fix_coordinates = \"True\"\n","if fix_coordinates == \"True\":\n","  shift = G.synthesis.input.affine(G.mapping.w_avg.unsqueeze(0))\n","  G.synthesis.input.affine.bias.data.add_(shift.squeeze(0))\n","  G.synthesis.input.affine.weight.data.zero_()"],"metadata":{"id":"xm6BfSZ8b36M"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###**Run the Model !**"],"metadata":{"id":"j2JskbnRdLPx"}},{"cell_type":"code","source":["from IPython.display import display\n","\n","tf = Compose([\n","  Resize(224),\n","  lambda x: torch.clamp((x+1)/2,min=0,max=1),\n","  ])\n","\n","def run(seed, G):\n","  torch.manual_seed(seed)\n","  timestring = time.strftime('%Y%m%d%H%M%S')\n","  rand_z = torch.from_numpy(np.random.RandomState(seed).randn(1, G.mapping.z_dim)).to(device)\n","  q = (G.mapping(rand_z, None, truncation_psi=0.2)) / w_stds\n","  q.requires_grad_()\n","\n","  with torch.no_grad():\n","    qs = []\n","    losses = []\n","    for _ in range(8):\n","      q = (G.mapping(torch.randn([4,G.mapping.z_dim], device=device), None, truncation_psi=0.7) - G.mapping.w_avg) / w_stds\n","      images = G.synthesis(q * w_stds + G.mapping.w_avg)\n","      embeds = embed_image(images.add(1).div(2))\n","      loss = spherical_dist_loss(embeds, target).mean(0)\n","      i = torch.argmin(loss)\n","      qs.append(q[i])\n","      losses.append(loss[i])\n","    qs = torch.stack(qs)\n","    losses = torch.stack(losses)\n","    i = torch.argmin(losses)\n","    q = qs[i].unsqueeze(0).requires_grad_()\n","\n","# Sampling loop\n","  q_ema = q\n","  opt = torch.optim.AdamW([q], lr=0.03, betas=(0.0,0.999))\n","  loop = tqdm(range(steps))\n","  for i in loop:\n","    opt.zero_grad()\n","    w = q * w_stds\n","    image = G.synthesis(w + G.mapping.w_avg, noise_mode='const')\n","    embed = embed_image(image.add(1).div(2))\n","    loss = spherical_dist_loss(embed, target).mean()\n","    loss.backward()\n","    opt.step()\n","    loop.set_postfix(loss=loss.item(), q_magnitude=q.std().item())\n","\n","    q_ema = q_ema * 0.9 + q * 0.1\n","    image = G.synthesis(q_ema * w_stds + G.mapping.w_avg, noise_mode='const')\n","\n","    if i % 10 == 0:\n","      display(TF.to_pil_image(tf(image)[0]))\n","    pil_image = TF.to_pil_image(image[0].add(1).div(2).clamp(0,1))\n","    os.makedirs(f'samples/{timestring}', exist_ok=True)\n","    pil_image.save(f'samples/{timestring}/{i:04}.jpg')\n","\n","  # Save images as a tar archive\n","  !tar cf samples/{timestring}.tar samples/{timestring}\n","\n","  return timestring\n","\n","timestring = run(seed, G)"],"metadata":{"id":"Br-0M9Kjdbte"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["base_url = \"https://api.ngc.nvidia.com/v2/models/nvidia/research/stylegan3/versions/1/files/\"\n","\n","model_name = \"stylegan3-r-afhqv2-512x512.pkl\"\n","network_url = base_url + model_name\n","\n","with open(fetch_model(network_url), 'rb') as fp:\n","  G = pickle.load(fp)['G_ema'].to(device)\n","\n","clip_model = CLIP()"],"metadata":{"id":"mQOxnhYQw-Qn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["text = 'Rhinoceros' # 이걸 바꿔줘도 됩니다!\n","target = clip_model.embed_text(text)\n","\n","steps = 100\n","\n","target_images = \"em.jpg\"\n","\n","seed =  -1  #-1 for completly random\n","if seed == -1:\n","    seed = np.random.randint(0,2**32 - 1)\n","\n","show_every_n_steps = 20\n","\n","zs = torch.randn([10000, G.mapping.z_dim], device=device)\n","w_stds = G.mapping(zs, None).std(0)\n","\n","fix_coordinates = \"True\"\n","if fix_coordinates == \"True\":\n","  shift = G.synthesis.input.affine(G.mapping.w_avg.unsqueeze(0))\n","  G.synthesis.input.affine.bias.data.add_(shift.squeeze(0))\n","  G.synthesis.input.affine.weight.data.zero_()"],"metadata":{"id":"1n_sV2Njw93G"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from IPython.display import display\n","\n","tf = Compose([\n","  Resize(224),\n","  lambda x: torch.clamp((x+1)/2,min=0,max=1),\n","  ])\n","\n","def run(seed, G):\n","  torch.manual_seed(seed)\n","  timestring = time.strftime('%Y%m%d%H%M%S')\n","  rand_z = torch.from_numpy(np.random.RandomState(seed).randn(1, G.mapping.z_dim)).to(device)\n","  q = (G.mapping(rand_z, None, truncation_psi=0.2)) / w_stds\n","  q.requires_grad_()\n","\n","  with torch.no_grad():\n","    qs = []\n","    losses = []\n","    for _ in range(8):\n","      q = (G.mapping(torch.randn([4,G.mapping.z_dim], device=device), None, truncation_psi=0.7) - G.mapping.w_avg) / w_stds\n","      images = G.synthesis(q * w_stds + G.mapping.w_avg)\n","      embeds = embed_image(images.add(1).div(2))\n","      loss = spherical_dist_loss(embeds, target).mean(0)\n","      i = torch.argmin(loss)\n","      qs.append(q[i])\n","      losses.append(loss[i])\n","    qs = torch.stack(qs)\n","    losses = torch.stack(losses)\n","    i = torch.argmin(losses)\n","    q = qs[i].unsqueeze(0).requires_grad_()\n","\n","# Sampling loop\n","  q_ema = q\n","  opt = torch.optim.AdamW([q], lr=0.03, betas=(0.0,0.999))\n","  loop = tqdm(range(steps))\n","  for i in loop:\n","    opt.zero_grad()\n","    w = q * w_stds\n","    image = G.synthesis(w + G.mapping.w_avg, noise_mode='const')\n","    embed = embed_image(image.add(1).div(2))\n","    loss = spherical_dist_loss(embed, target).mean()\n","    loss.backward()\n","    opt.step()\n","    loop.set_postfix(loss=loss.item(), q_magnitude=q.std().item())\n","\n","    q_ema = q_ema * 0.9 + q * 0.1\n","    image = G.synthesis(q_ema * w_stds + G.mapping.w_avg, noise_mode='const')\n","\n","    if i % 10 == 0:\n","      display(TF.to_pil_image(tf(image)[0]))\n","    pil_image = TF.to_pil_image(image[0].add(1).div(2).clamp(0,1))\n","    os.makedirs(f'samples/{timestring}', exist_ok=True)\n","    pil_image.save(f'samples/{timestring}/{i:04}.jpg')\n","\n","  # Save images as a tar archive\n","  !tar cf samples/{timestring}.tar samples/{timestring}\n","\n","  return timestring\n","\n","timestring = run(seed, G)"],"metadata":{"id":"GAWYAGwhxL4j"},"execution_count":null,"outputs":[]}]}
